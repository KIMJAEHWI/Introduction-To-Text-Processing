{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim Tutorial\n",
    "=============\n",
    "gensim: https://radimrehurek.com/gensim/\n",
    "\n",
    "gensim API: https://radimrehurek.com/gensim/apiref.html#api-reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Core Concepts\n",
    "=============\n",
    "\n",
    "\n",
    "This tutorial introduces Documents, Corpora, Vectors and Models: the basic concepts and terms needed to understand and use gensim.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core concepts of ``gensim`` are:\n",
    "\n",
    "1. `core_concepts_document`: some text.\n",
    "2. `core_concepts_corpus`: a collection of documents.\n",
    "3. `core_concepts_vector`: a mathematically convenient representation of a document.\n",
    "4. `core_concepts_model`: an algorithm for transforming vectors from one representation to another.\n",
    "\n",
    "Let's examine each of these in slightly more detail.\n",
    "\n",
    "\n",
    "Document\n",
    "--------\n",
    "\n",
    "In Gensim, a *document* is an object of the [text sequence type](https://docs.python.org/3.7/library/stdtypes.html#text-sequence-type-str)_ (commonly known as ``str`` in Python 3).\n",
    "A document could be anything from a short 280 character tweet, a single\n",
    "paragraph (i.e., journal article abstract), a news article, or a book.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"Human machine interface for lab abc computer applications\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Corpus\n",
    "------\n",
    "\n",
    "A *corpus* is a collection of `core_concepts_document` objects.\n",
    "Corpora serve two roles in Gensim:\n",
    "\n",
    "1. Input for training a `core_concepts_model`.\n",
    "   During training, the models use this *training corpus* to look for common\n",
    "   themes and topics, initializing their internal model parameters.\n",
    "\n",
    "   Gensim focuses on *unsupervised* models so that no human intervention,\n",
    "   such as costly annotations or tagging documents by hand, is required.\n",
    "\n",
    "2. Documents to organize.\n",
    "   After training, a topic model can be used to extract topics from new\n",
    "   documents (documents not seen in the training corpus).\n",
    "\n",
    "   Such corpora can be indexed for [Similarity Queries](https://radimrehurek.com/gensim/auto_examples/core/run_similarity_queries.html#sphx-glr-auto-examples-core-run-similarity-queries-py)\n",
    "   queried by semantic similarity, clustered etc.\n",
    "\n",
    "Here is an example corpus.\n",
    "It consists of 9 documents, where each document is a string consisting of a single sentence.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]\n",
    "# nine documents: each documents is actually a sentence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The above example loads the entire corpus into memory.\n",
    "In practice, corpora may be very large, so loading them into memory may be impossible.\n",
    "Gensim intelligently handles such corpora by *streaming* them one document at a time.\n",
    "See [Corpus Streaming – One Document at a Time](https://radimrehurek.com/gensim/auto_examples/core/run_corpora_and_vector_spaces.html#corpus-streaming-tutorial) for details.\n",
    "\n",
    "This is a particularly small example of a corpus for illustration purposes.\n",
    "Another example could be a list of all the plays written by Shakespeare, list\n",
    "of all wikipedia articles, or all tweets by a particular person of interest.\n",
    "\n",
    "After collecting our corpus, there are typically a number of preprocessing\n",
    "steps we want to undertake. We'll keep it simple and just remove some\n",
    "commonly used English words (such as 'the') and words that occur only once in\n",
    "the corpus. In the process of doing so, we'll tokenize our data.\n",
    "Tokenization breaks up the documents into words (in this case using space as\n",
    "a delimiter).\n",
    "\n",
    "\n",
    "There are better ways to perform preprocessing than just lower-casing and\n",
    "splitting by space.  Effective preprocessing is beyond the scope of this\n",
    "tutorial: if you're interested, check out the\n",
    "[gensim.utils.simple_preprocess()](https://radimrehurek.com/gensim/utils.html#gensim.utils.simple_preprocess) function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing\n",
    "\n",
    "# Create a set of frequent words\n",
    "stoplist = set('for a of the and to in'.split(' '))\n",
    "\n",
    "# Lowercase each document, split it by white space and filter out stopwords\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in text_corpus]\n",
    "\n",
    "# Count word frequencies\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Only keep words that appear more than once\n",
    "processed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "pprint.pprint(processed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding, we want to associate each word in the corpus with a unique\n",
    "integer ID. We can do this using the `gensim.corpora.Dictionary`\n",
    "class.  This dictionary defines the vocabulary of all words that our\n",
    "processing knows about.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.1.2-cp38-cp38-win_amd64.whl (24.0 MB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Using cached smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\82109\\anaconda3\\lib\\site-packages (from gensim) (0.29.23)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\82109\\anaconda3\\lib\\site-packages (from gensim) (1.20.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\82109\\anaconda3\\lib\\site-packages (from gensim) (1.6.2)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.1.2 smart-open-5.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)\n",
    "\n",
    "# reason we bulid dictionary: identify total number of words, which we say vocabularly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our corpus is small, there are only 12 different tokens in this\n",
    "`gensim.corpora.Dictionary`. For larger corpuses, dictionaries that\n",
    "contains hundreds of thousands of tokens are quite common.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Vector\n",
    "------\n",
    "\n",
    "To infer the latent structure in our corpus we need a way to represent\n",
    "documents that we can manipulate mathematically. One approach is to represent\n",
    "each document as a vector of *features*.\n",
    "For example, a single feature may be thought of as a question-answer pair:\n",
    "\n",
    "1. How many times does the word *splonge* appear in the document? Zero.\n",
    "2. How many paragraphs does the document consist of? Two.\n",
    "3. How many fonts does the document use? Five.\n",
    "\n",
    "The question is usually represented only by its integer id (such as `1`, `2` and `3`).\n",
    "The representation of this document then becomes a series of pairs like ``(1, 0.0), (2, 2.0), (3, 5.0)``.\n",
    "This is known as a *dense vector*, because it contains an explicit answer to each of the above questions.\n",
    "\n",
    "If we know all the questions in advance, we may leave them implicit\n",
    "and simply represent the document as ``(0, 2, 5)``.\n",
    "This sequence of answers is the **vector** for our document (in this case a 3-dimensional dense vector).\n",
    "For practical purposes, only questions to which the answer is (or\n",
    "can be converted to) a *single floating point number* are allowed in Gensim.\n",
    "\n",
    "In practice, vectors often consist of many zero values.\n",
    "To save memory, Gensim omits all vector elements with value 0.0.\n",
    "The above example thus becomes ``(2, 2.0), (3, 5.0)``.\n",
    "This is known as a *sparse vector* or *bag-of-words vector*.\n",
    "The values of all missing features in this sparse representation can be unambiguously resolved to zero, ``0.0``.\n",
    "\n",
    "Assuming the questions are the same, we can compare the vectors of two different documents to each other.\n",
    "For example, assume we are given two vectors ``(0.0, 2.0, 5.0)`` and ``(0.1, 1.9, 4.9)``.\n",
    "Because the vectors are very similar to each other, we can conclude that the documents corresponding to those vectors are similar, too.\n",
    "Of course, the correctness of that conclusion depends on how well we picked the questions in the first place.\n",
    "\n",
    "Another approach to represent a document as a vector is the *bag-of-words\n",
    "model*.\n",
    "Under the bag-of-words model each document is represented by a vector\n",
    "containing the frequency counts of each word in the dictionary.\n",
    "For example, assume we have a dictionary containing the words\n",
    "``['coffee', 'milk', 'sugar', 'spoon']``.\n",
    "A document consisting of the string ``\"coffee milk coffee\"`` would then\n",
    "be represented by the vector ``[2, 1, 0, 0]`` where the entries of the vector\n",
    "are (in order) the occurrences of \"coffee\", \"milk\", \"sugar\" and \"spoon\" in\n",
    "the document. The length of the vector is the number of entries in the\n",
    "dictionary. One of the main properties of the bag-of-words model is that it\n",
    "completely ignores the order of the tokens in the document that is encoded,\n",
    "which is where the name bag-of-words comes from.\n",
    "\n",
    "Our processed corpus has 12 unique words in it, which means that each\n",
    "document will be represented by a 12-dimensional vector under the\n",
    "bag-of-words model. \n",
    "\n",
    "We can use the dictionary to turn tokenized documents\n",
    "into these 12-dimensional vectors. We can see what these IDs correspond to:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, suppose we wanted to vectorize the phrase \"Human computer\n",
    "interaction\" (note that this phrase was not in our original corpus). We can\n",
    "create the bag-of-word representation for a document using the ``doc2bow``\n",
    "method of the dictionary, which returns a sparse representation of the word\n",
    "counts:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = \"Human computer interaction\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first entry in each tuple corresponds to the ID of the token in the\n",
    "dictionary, the second corresponds to the count of this token.\n",
    "\n",
    "Note that \"interaction\" did not occur in the original corpus and so it was\n",
    "not included in the vectorization. Also note that this vector only contains\n",
    "entries for words that actually appeared in the document. Because any given\n",
    "document will only contain a few words out of the many words in the\n",
    "dictionary, words that do not appear in the vectorization are represented as\n",
    "implicitly zero as a space saving measure.\n",
    "\n",
    "We can convert our entire original corpus to a list of vectors:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "pprint.pprint(bow_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while this list lives entirely in memory, in most applications you\n",
    "will want a more scalable solution. Luckily, ``gensim`` allows you to use any\n",
    "iterator that returns a single document vector at a time. See the\n",
    "documentation for more details.\n",
    "\n",
    "\n",
    "The distinction between a document and a vector is that the former is text,\n",
    "and the latter is a mathematically convenient representation of the text.\n",
    "Sometimes, people will use the terms interchangeably: for example, given\n",
    "some arbitrary document ``D``, instead of saying \"the vector that\n",
    "corresponds to document ``D``\", they will just say \"the vector ``D``\" or\n",
    "the \"document ``D``\".  This achieves brevity at the cost of ambiguity.\n",
    "\n",
    "As long as you remember that documents exist in document space, and that\n",
    "vectors exist in vector space, the above ambiguity is acceptable.\n",
    "\n",
    "\n",
    "Depending on how the representation was obtained, two different documents\n",
    "may have the same vector representations.\n",
    "\n",
    "\n",
    "Model\n",
    "-----\n",
    "\n",
    "Now that we have vectorized our corpus we can begin to transform it using\n",
    "*models*. We use model as an abstract term referring to a *transformation* from\n",
    "one document representation to another. In ``gensim`` documents are\n",
    "represented as vectors so a model can be thought of as a transformation\n",
    "between two vector spaces. The model learns the details of this\n",
    "transformation during training, when it reads the training\n",
    "`core_concepts_corpus`.\n",
    "\n",
    "One simple example of a model is [tf-idf](\n",
    "<https://en.wikipedia.org/wiki/Tf%E2%80%93idf>).  The tf-idf model\n",
    "transforms vectors from the bag-of-words representation to a vector space\n",
    "where the frequency counts are weighted according to the relative rarity of\n",
    "each word in the corpus.\n",
    "\n",
    "Here's a simple example. Let's initialize the tf-idf model, training it on\n",
    "our corpus and transforming the string \"system minors\":\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "# train the model\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# transform the \"system minors\" string\n",
    "words = \"system minors\".lower().split()\n",
    "print(tfidf[dictionary.doc2bow(words)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``tfidf`` model again returns a list of tuples, where the first entry is\n",
    "the token ID and the second entry is the tf-idf weighting. Note that the ID\n",
    "corresponding to \"system\" (which occurred 4 times in the original corpus) has\n",
    "been weighted lower than the ID corresponding to \"minors\" (which only\n",
    "occurred twice).\n",
    "\n",
    "You can save trained models to disk and later load them back, either to\n",
    "continue training on new training documents or to transform new documents.\n",
    "\n",
    "``gensim`` offers a number of different models/transformations.\n",
    "For more, see [Topics and Transformations](https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html#sphx-glr-auto-examples-core-run-topics-and-transformations-py).\n",
    "\n",
    "Once you've created the model, you can do all sorts of cool stuff with it.\n",
    "For example, to transform the whole corpus via TfIdf and index it, in\n",
    "preparation for similarity queries:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and to query the similarity of our query document ``query_document`` against every document in the corpus:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_document = 'system engineering'.split()\n",
    "query_bow = dictionary.doc2bow(query_document)\n",
    "sims = index[tfidf[query_bow]]\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to read this output?\n",
    "Document 3 has a similarity score of 0.718=72%, document 2 has a similarity score of 42% etc.\n",
    "We can make this slightly more readable by sorting:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n",
    "    print(document_number, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Corpora and Vector Spaces\n",
    "\n",
    "Demonstrates transforming text into a vector space representation.\n",
    "\n",
    "Also introduces corpus streaming and persistence to disk in various formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let’s create a small corpus of nine short documents.\n",
    "\n",
    "\n",
    "## From Strings to Vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a tiny corpus of nine documents, each consisting of only a single sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "dictionary.save('file/deerwester.dict')  # store the dictionary, for future reference\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we assigned a unique integer id to all words appearing in the corpus with the\n",
    "`gensim.corpora.dictionary.Dictionary` class. This sweeps across the texts, collecting word counts\n",
    "and relevant statistics. In the end, we see there are twelve distinct words in the\n",
    "processed corpus, which means each document will be represented by twelve numbers (ie., by a 12-D vector).\n",
    "To see the mapping between words and their ids:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually convert tokenized documents to vectors:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_doc = \"Human computer interaction\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)  # the word \"interaction\" does not appear in the dictionary and is ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `doc2bow` simply counts the number of occurrences of\n",
    "each distinct word, converts the word to its integer word id\n",
    "and returns the result as a sparse vector. The sparse vector ``[(0, 1), (1, 1)]``\n",
    "therefore reads: in the document `\"Human computer interaction\"`, the words `computer`\n",
    "(id 0) and `human` (id 1) appear once; the other ten dictionary words appear (implicitly) zero times.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize('file/deerwester.mm', corpus)  # store to disk, for later use\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now it should be clear that the vector feature with ``id=10`` stands for the question \"How many\n",
    "times does the word `graph` appear in the document?\" and that the answer is \"zero\" for\n",
    "the first six documents and \"one\" for the remaining three.\n",
    "\n",
    "\n",
    "## Corpus Streaming -- One Document at a Time\n",
    "\n",
    "Note that `corpus` above resides fully in memory, as a plain Python list.\n",
    "In this simple example, it doesn't matter much, but just to make things clear,\n",
    "let's assume there are millions of documents in the corpus. Storing all of them in RAM won't do.\n",
    "Instead, let's assume the documents are stored in a file on disk, one document per line. Gensim\n",
    "only requires that a corpus must be able to return one document vector at a time:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smart_open import open  # for transparently opening remote files\n",
    "\n",
    "\n",
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        for line in open('https://radimrehurek.com/mycorpus.txt'):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield dictionary.doc2bow(line.lower().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full power of Gensim comes from the fact that a corpus doesn't have to be\n",
    "a ``list``, or a ``NumPy`` array, or a ``Pandas`` dataframe, or whatever.\n",
    "Gensim *accepts any object that, when iterated over, successively yields\n",
    "documents*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This flexibility allows you to create your own corpus classes that stream the\n",
    "documents directly from disk, network, database, dataframes... The models\n",
    "in Gensim are implemented such that they don't require all vectors to reside\n",
    "in RAM at once. You can even create the documents on the fly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the sample [mycorpus.txt](https://radimrehurek.com/mycorpus.txt) file. The assumption that\n",
    "each document occupies one line in a single file is not important; you can mold\n",
    "the `__iter__` function to fit your input format, whatever it is.\n",
    "Walking directories, parsing XML, accessing the network...\n",
    "Just parse your input to retrieve a clean list of tokens in each document,\n",
    "then convert the tokens via a dictionary to their ids and yield the resulting sparse vector inside `__iter__`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_memory_friendly = MyCorpus()  # doesn't load the corpus into memory!\n",
    "print(corpus_memory_friendly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus is now an object. We didn't define any way to print it, so `print` just outputs address\n",
    "of the object in memory. Not very useful. To see the constituent vectors, let's\n",
    "iterate over the corpus and print each document vector (one at a time):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vector in corpus_memory_friendly:  # load one vector into memory at a time\n",
    "    print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the output is the same as for the plain Python list, the corpus is now much\n",
    "more memory friendly, because at most one vector resides in RAM at a time. Your\n",
    "corpus can now be as large as you want.\n",
    "\n",
    "Similarly, to construct the dictionary without loading all texts into memory:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect statistics about all tokens\n",
    "dictionary = corpora.Dictionary(line.lower().split() for line in open('https://radimrehurek.com/mycorpus.txt'))\n",
    "# remove stop words and words that appear only once\n",
    "stop_ids = [\n",
    "    dictionary.token2id[stopword]\n",
    "    for stopword in stoplist\n",
    "    if stopword in dictionary.token2id\n",
    "]\n",
    "once_ids = [tokenid for tokenid, docfreq in dictionary.dfs.items() if docfreq == 1]\n",
    "dictionary.filter_tokens(stop_ids + once_ids)  # remove stop words and words that appear only once\n",
    "dictionary.compactify()  # remove gaps in id sequence after words that were removed\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that is all there is to it! At least as far as bag-of-words representation is concerned.\n",
    "Of course, what we do with such a corpus is another question; it is not at all clear\n",
    "how counting the frequency of distinct words could be useful. As it turns out, it isn't, and\n",
    "we will need to apply a transformation on this simple representation first, before\n",
    "we can use it to compute any meaningful document vs. document similarities.\n",
    "\n",
    "## Corpus Formats\n",
    "\n",
    "There exist several file formats for serializing a Vector Space corpus (~sequence of vectors) to disk.\n",
    "`Gensim` implements them via the *streaming corpus interface* mentioned earlier:\n",
    "documents are read from (resp. stored to) disk in a lazy fashion, one document at\n",
    "a time, without the whole corpus being read into main memory at once.\n",
    "\n",
    "One of the more notable file formats is the [Market Matrix format](http://math.nist.gov/MatrixMarket/formats.html).\n",
    "To save a corpus in the Matrix Market format:\n",
    "\n",
    "create a toy corpus of 2 documents, as a plain Python list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [[(1, 0.5)], []]  # make one document empty, for the heck of it\n",
    "\n",
    "corpora.MmCorpus.serialize('file/corpus.mm', corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other formats include [Joachim's SVMlight format](http://svmlight.joachims.org),\n",
    "[Blei's LDA-C format](https://radimrehurek.com/gensim/corpora/bleicorpus.html), and\n",
    "[GibbsLDA++ format](http://gibbslda.sourceforge.net/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora.SvmLightCorpus.serialize('file/corpus.svmlight', corpus)\n",
    "corpora.BleiCorpus.serialize('file/corpus.lda-c', corpus)\n",
    "corpora.LowCorpus.serialize('file/corpus.low', corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, to load a corpus iterator from a Matrix Market file:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpora.MmCorpus('file/corpus.mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus objects are streams, so typically you won't be able to print them directly:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, to view the contents of a corpus:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one way of printing a corpus: load it entirely into memory\n",
    "print(list(corpus))  # calling list() will convert any sequence to a plain Python list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way of doing it: print one document at a time, making use of the streaming interface\n",
    "for doc in corpus:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second way is obviously more memory-friendly, but for testing and development\n",
    "purposes, nothing beats the simplicity of calling ``list(corpus)``.\n",
    "\n",
    "To save the same Matrix Market document stream in Blei's LDA-C format,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora.BleiCorpus.serialize('file/corpus.lda-c', corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this way, `gensim` can also be used as a memory-efficient **I/O format conversion tool**:\n",
    "just load a document stream using one format and immediately save it in another format.\n",
    "Adding new formats is dead easy, check out the [code for the SVMlight corpus](https://github.com/piskvorky/gensim/blob/develop/gensim/corpora/svmlightcorpus.py) for an example.\n",
    "\n",
    "## Compatibility with NumPy and SciPy\n",
    "\n",
    "Gensim also contains [efficient utility functions](http://radimrehurek.com/gensim/matutils.html)\n",
    "to help converting from/to numpy matrices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "numpy_matrix = np.random.randint(10, size=[5, 2])  # random matrix as an example\n",
    "corpus = gensim.matutils.Dense2Corpus(numpy_matrix)\n",
    "# numpy_matrix = gensim.matutils.corpus2dense(corpus, num_terms=number_of_corpus_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and from/to `scipy.sparse` matrices\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "scipy_sparse_matrix = scipy.sparse.random(5, 2)  # random sparse matrix as example\n",
    "corpus = gensim.matutils.Sparse2Corpus(scipy_sparse_matrix)\n",
    "#scipy_csc_matrix = gensim.matutils.corpus2csc(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Topics and Transformations\n",
    "===========================\n",
    "\n",
    "Introduces transformations and demonstrates their use on a toy corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, I will show how to transform documents from one vector representation\n",
    "into another. This process serves two goals:\n",
    "\n",
    "1. To bring out hidden structure in the corpus, discover relationships between\n",
    "   words and use them to describe the documents in a new and\n",
    "   (hopefully) more semantic way.\n",
    "2. To make the document representation more compact. This both improves efficiency\n",
    "   (new representation consumes less resources) and efficacy (marginal data\n",
    "   trends are ignored, noise-reduction).\n",
    "\n",
    "Creating the Corpus\n",
    "-------------------\n",
    "\n",
    "First, we need to create a corpus to work with.\n",
    "This step is the same as in the previous tutorial;\n",
    "if you completed it, feel free to skip to the next section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a transformation\n",
    "\n",
    "\n",
    "The transformations are standard Python objects, typically initialized by means of training corpus:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)  # step 1 -- initialize a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used our old corpus from tutorial 1 to initialize (train) the transformation model. Different\n",
    "transformations may require different initialization parameters; in case of TfIdf, the\n",
    "\"training\" consists simply of going through the supplied corpus once and computing document frequencies\n",
    "of all its features. Training other models, such as Latent Semantic Analysis or Latent Dirichlet\n",
    "Allocation, is much more involved and, consequently, takes much more time.\n",
    "\n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>Transformations always convert between two specific vector\n",
    "  spaces. The same vector space (= the same set of feature ids) must be used for training\n",
    "  as well as for subsequent vector transformations. Failure to use the same input\n",
    "  feature space, such as applying a different string preprocessing, using different\n",
    "  feature ids, or using bag-of-words input vectors where TfIdf vectors are expected, will\n",
    "  result in feature mismatch during transformation calls and consequently in either\n",
    "  garbage output and/or runtime exceptions.</p></div>\n",
    "\n",
    "\n",
    "### Transforming vectors\n",
    "\n",
    "\n",
    "From now on, ``tfidf`` is treated as a read-only object that can be used to convert\n",
    "any vector from the old representation (bag-of-words integer counts) to the new representation\n",
    "(TfIdf real-valued weights):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bow = [(0, 1), (1, 1)]\n",
    "print(tfidf[doc_bow])  # step 2 -- use the model to transform vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or to apply a transformation to a whole corpus:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]\n",
    "for doc in corpus_tfidf:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular case, we are transforming the same corpus that we used\n",
    "for training, but this is only incidental. Once the transformation model has been initialized,\n",
    "it can be used on any vectors (provided they come from the same vector space, of course),\n",
    "even if they were not used in the training corpus at all. \n",
    "<div class=\"alert alert-info\"><h4>Note</h4><p>Calling <code>model[corpus]</code> only creates a wrapper around the old <code>corpus</code> document stream -- actual conversions are done on-the-fly, during document iteration.\n",
    "  We cannot convert the entire corpus at the time of calling <code>corpus_transformed = model[corpus]</code>,\n",
    "  because that would mean storing the result in main memory, and that contradicts gensim's objective of memory-indepedence.\n",
    "  If you will be iterating over the transformed <code>corpus_transformed</code> multiple times, and the\n",
    "  transformation is costly, serialize the resulting corpus to disk first <corpus-formats> and continue\n",
    "  using that.</p></div>\n",
    "\n",
    "Transformations can also be serialized, one on top of another, in a sort of chain:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)  # initialize an LSI transformation\n",
    "corpus_lsi = lsi_model[corpus_tfidf]  # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we transformed our Tf-Idf corpus via [Latent Semantic Indexing](http://en.wikipedia.org/wiki/Latent_semantic_indexing>)\n",
    "into a latent 2-D space (2-D because we set ``num_topics=2``). Now you're probably wondering: what do these two latent\n",
    "dimensions stand for? Let's inspect with <code>models.LsiModel.print_topics</code>:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsi_model.print_topics(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(the topics are printed to log -- see the note at the top of this page about activating\n",
    "logging)\n",
    "\n",
    "It appears that according to LSI, \"trees\", \"graph\" and \"minors\" are all related\n",
    "words (and contribute the most to the direction of the first topic), while the\n",
    "second topic practically concerns itself with all the other words. As expected,\n",
    "the first five documents are more strongly related to the second topic while the\n",
    "remaining four documents to the first topic:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "for doc, as_text in zip(corpus_lsi, text_corpus):\n",
    "    print(doc, as_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model persistency is achieved with the `save` and `load` functions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'file/model-lsi.lsi'\n",
    "lsi_model.save(model_path)  # same for tfidf, lda, ...\n",
    "loaded_lsi_model = models.LsiModel.load(model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Available transformations\n",
    "--------------------------\n",
    "\n",
    "Gensim implements several popular Vector Space Model algorithms:\n",
    "\n",
    "* [Term Frequency * Inverse Document Frequency, Tf-Idf](http://en.wikipedia.org/wiki/Tf%E2%80%93idf>)\n",
    "  expects a bag-of-words (integer values) training corpus during initialization.\n",
    "  During transformation, it will take a vector and return another vector of the\n",
    "  same dimensionality, except that features which were rare in the training corpus\n",
    "  will have their value increased.\n",
    "  It therefore converts integer-valued vectors into real-valued ones, while leaving\n",
    "  the number of dimensions intact. It can also optionally normalize the resulting\n",
    "  vectors to (Euclidean) unit length.\n",
    "<code>model = models.TfidfModel(corpus, normalize=True)</code>\n",
    "\n",
    "* [Latent Semantic Indexing, LSI (or sometimes LSA)](http://en.wikipedia.org/wiki/Latent_semantic_indexing>)\n",
    "  transforms documents from either bag-of-words or (preferrably) TfIdf-weighted space into\n",
    "  a latent space of a lower dimensionality. For the toy corpus above we used only\n",
    "  2 latent dimensions, but on real corpora, target dimensionality of 200--500 is recommended\n",
    "  as a \"golden standard\" [1]_.\n",
    "<code>model = models.LsiModel(tfidf_corpus, id2word=dictionary, num_topics=300)</code>\n",
    "\n",
    "* [Random Projections, RP](http://www.cis.hut.fi/ella/publications/randproj_kdd.pdf) aim to\n",
    "  reduce vector space dimensionality. This is a very efficient (both memory- and\n",
    "  CPU-friendly) approach to approximating TfIdf distances between documents, by throwing in a little randomness.\n",
    "  Recommended target dimensionality is again in the hundreds/thousands, depending on your dataset.\n",
    "  <code>model = models.RpModel(tfidf_corpus, num_topics=500)</code>\n",
    "\n",
    "* [Latent Dirichlet Allocation, LDA](http://en.wikipedia.org/wiki/Latent_Dirichlet_allocation>)\n",
    "  is yet another transformation from bag-of-words counts into a topic space of lower\n",
    "  dimensionality. LDA is a probabilistic extension of LSA (also called multinomial PCA),\n",
    "  so LDA's topics can be interpreted as probability distributions over words. These distributions are,\n",
    "  just like with LSA, inferred automatically from a training corpus. Documents\n",
    "  are in turn interpreted as a (soft) mixture of these topics (again, just like with LSA).\n",
    "    <code>model = models.LdaModel(corpus, id2word=dictionary, num_topics=100)</code>\n",
    "\n",
    "* [Hierarchical Dirichlet Process, HDP](http://jmlr.csail.mit.edu/proceedings/papers/v15/wang11a/wang11a.pdf>)\n",
    "  is a non-parametric bayesian method (note the missing number of requested topics):\n",
    "<code>model = models.HdpModel(corpus, id2word=dictionary)</code>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Similarity Queries\n",
    "==================\n",
    "\n",
    "Demonstrates querying a corpus for similar documents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the Corpus\n",
    "-------------------\n",
    "\n",
    "First, we need to create a corpus to work with.\n",
    "This step is the same as in the previous tutorial;\n",
    "if you completed it, feel free to skip to the next section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity interface\n",
    "--------------------\n",
    "\n",
    "In the previous tutorials, we covered what it means to create a corpus in the Vector Space Model and how\n",
    "to transform it between different vector spaces. A common reason for such a\n",
    "charade is that we want to determine **similarity between pairs of\n",
    "documents**, or the **similarity between a specific document and a set of\n",
    "other documents** (such as a user query vs. indexed documents).\n",
    "\n",
    "We first use this tiny corpus to define a 2-dimensional\n",
    "LSI space:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "lsi = models.LsiModel(bow_corpus, id2word=dictionary, num_topics=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purposes of this tutorial, there are only two things you need to know about LSI.\n",
    "First, it's just another transformation: it transforms vectors from one space to another.\n",
    "Second, the benefit of LSI is that enables identifying patterns and relationships between terms (in our case, words in a document) and topics.\n",
    "Our LSI space is two-dimensional (`num_topics = 2`) so there are two topics, but this is arbitrary.\n",
    "If you're interested, you can read more about LSI here: [Latent Semantic Indexing](https://en.wikipedia.org/wiki/Latent_semantic_indexing>):\n",
    "\n",
    "Now suppose a user typed in the query `\"Human computer interaction\"`. We would\n",
    "like to sort our nine corpus documents in decreasing order of relevance to this query.\n",
    "Unlike modern search engines, here we only concentrate on a single aspect of possible\n",
    "similarities---on apparent semantic relatedness of their texts (words). No hyperlinks,\n",
    "no random-walk static ranks, just a semantic extension over the boolean keyword match:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"Human computer interaction\"\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow]  # convert the query to LSI space\n",
    "print(vec_lsi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, we will be considering [cosine similarity](http://en.wikipedia.org/wiki/Cosine_similarity>)\n",
    "in Vector Space Modeling, but wherever the vectors represent probability distributions,\n",
    "[different similarity measures](http://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence#Symmetrised_divergence>)\n",
    "may be more appropriate.\n",
    "\n",
    "#### Initializing query structures\n",
    "\n",
    "\n",
    "To prepare for similarity queries, we need to enter all documents which we want\n",
    "to compare against subsequent queries. In our case, they are the same nine documents\n",
    "used for training LSI, converted to 2-D LSA space. But that's only incidental, we\n",
    "might also be indexing a different corpus altogether.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "index = similarities.MatrixSimilarity(lsi[bow_corpus])  # transform corpus to LSI space and index it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Index persistency is handled via the standard `save` and `load` functions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.save('file/deerwester.index')\n",
    "index = similarities.MatrixSimilarity.load('file/deerwester.index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Performing querie\n",
    "To obtain similarities of our query document against the nine indexed documents:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = index[vec_lsi]  # perform a similarity query against the corpus\n",
    "print(list(enumerate(sims)))  # print (document_number, document_similarity) 2-tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine measure returns similarities in the range `<-1, 1>` (the greater, the more similar),\n",
    "so that the first document has a score of 0.99809301 etc.\n",
    "\n",
    "With some standard Python magic we sort these similarities into descending\n",
    "order, and obtain the final answer to the query `\"Human computer interaction\"`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = sorted(enumerate(sims), key=lambda item: item[1], reverse=True)\n",
    "for doc_position, doc_score in sims:\n",
    "    print(doc_score, text_corpus[doc_position])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing to note here is that documents no. 2 (``\"The EPS user interface management system\"``)\n",
    "and 4 (``\"Relation of user perceived response time to error measurement\"``) would never be returned by\n",
    "a standard boolean fulltext search, because they do not share any common words with ``\"Human computer interaction\"``. However, after applying LSI, we can observe that both of\n",
    "them received quite high similarity scores (no. 2 is actually the most similar!),\n",
    "which corresponds better to our intuition of\n",
    "them sharing a \"computer-human\" related topic with the query. In fact, this semantic\n",
    "generalization is the reason why we apply transformations and do topic modelling\n",
    "in the first place.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Word2Vec Model\n",
    "==============\n",
    "\n",
    "Introduces Gensim's Word2Vec model and demonstrates its use on the [Lee Evaluation Corpus](https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf>)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec is a widely used algorithm based on neural\n",
    "networks, commonly referred to as \"deep learning\" (though word2vec itself is rather shallow).\n",
    "Using large amounts of unannotated plain text, word2vec learns relationships\n",
    "between words automatically. The output are vectors, one vector per word.\n",
    "\n",
    "\n",
    "This tutorial:\n",
    "\n",
    "* Introduces ``Word2Vec`` as an improvement over traditional bag-of-words\n",
    "* Shows off a demo of ``Word2Vec`` using a pre-trained model\n",
    "* Demonstrates training a new model from your own data\n",
    "* Demonstrates loading and saving models\n",
    "* Introduces several training parameters and demonstrates their effect\n",
    "* Discusses memory requirements\n",
    "* Visualizes Word2Vec embeddings by applying dimensionality reduction\n",
    "\n",
    "Review: Bag-of-words\n",
    "--------------------\n",
    "The bag-of-words model transforms each document to a fixed-length vector of integers.\n",
    "For example, given the sentences:\n",
    "\n",
    "- ``John likes to watch movies. Mary likes movies too.``\n",
    "- ``John also likes to watch football games. Mary hates football.``\n",
    "\n",
    "The model outputs the vectors:\n",
    "\n",
    "- ``[1, 2, 1, 1, 2, 1, 1, 0, 0, 0, 0]``\n",
    "- ``[1, 1, 1, 1, 0, 1, 0, 1, 2, 1, 1]``\n",
    "\n",
    "Each vector has 10 elements, where each element counts the number of times a\n",
    "particular word occurred in the document.\n",
    "The order of elements is arbitrary.\n",
    "In the example above, the order of the elements corresponds to the words:\n",
    "``[\"John\", \"likes\", \"to\", \"watch\", \"movies\", \"Mary\", \"too\", \"also\", \"football\", \"games\", \"hates\"]``.\n",
    "\n",
    "Bag-of-words models are surprisingly effective, but have several weaknesses.\n",
    "\n",
    "First, they lose all information about word order.\n",
    "\n",
    "Second, the model does not attempt to learn the meaning of the underlying\n",
    "words, and as a consequence, the distance between vectors doesn't always\n",
    "reflect the difference in meaning.  The ``Word2Vec`` model addresses this\n",
    "second problem.\n",
    "\n",
    "Introducing: the ``Word2Vec`` Model\n",
    "-----------------------------------\n",
    "\n",
    "``Word2Vec`` is a more recent model that embeds words in a lower-dimensional\n",
    "vector space using a shallow neural network. The result is a set of\n",
    "word-vectors where vectors close together in vector space have similar\n",
    "meanings based on context, and word-vectors distant to each other have\n",
    "differing meanings. For example, ``strong`` and ``powerful`` would be close\n",
    "together and ``strong`` and ``Paris`` would be relatively far.\n",
    "\n",
    "The are two versions of this model and :py:class:`~gensim.models.word2vec.Word2Vec`\n",
    "class implements them both:\n",
    "\n",
    "1. Skip-grams (SG)\n",
    "2. Continuous-bag-of-words (CBOW)\n",
    "\n",
    "\n",
    "The Word2Vec Skip-gram model, for example, takes in pairs (word1, word2) generated by moving a\n",
    "window across text data, and trains a 1-hidden-layer neural network based on\n",
    "the synthetic task of given an input word, giving us a predicted probability\n",
    "distribution of nearby words to the input. A virtual one-hot encoding of words goes through a 'projection layer' to the hidden layer; these projection\n",
    "weights are later interpreted as the word embeddings. So if the hidden layer\n",
    "has 300 neurons, this network will give us 300-dimensional word embeddings.\n",
    "\n",
    "Continuous-bag-of-words Word2vec is very similar to the skip-gram model. It\n",
    "is also a 1-hidden-layer neural network. The synthetic training task now uses\n",
    "the average of multiple input context words, rather than a single word as in\n",
    "skip-gram, to predict the center word. Again, the projection weights that\n",
    "turn one-hot words into averageable vectors, of the same width as the hidden\n",
    "layer, are interpreted as the word embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec Demo\n",
    "-------------\n",
    "\n",
    "To see what ``Word2Vec`` can do, let's download a pre-trained model and play\n",
    "around with it. We will fetch the Word2Vec model trained on part of the\n",
    "Google News dataset, covering approximately 3 million words and phrases. Such\n",
    "a model can take hours to train, but since it's already available,\n",
    "downloading and loading it with Gensim takes minutes.\n",
    "\n",
    "The model is approximately 2GB, so you'll need a decent network connection\n",
    "to proceed.  Otherwise, skip ahead to the \"Training Your Own Model\" section\n",
    " below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A common operation is to retrieve the vocabulary of a model. That is trivial:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, word in enumerate(wv.index_to_key):\n",
    "    if index == 10:\n",
    "        break\n",
    "    print(f\"word #{index}/{len(wv.index_to_key)} is {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily obtain vectors for terms the model is familiar with:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_king = wv['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vec_king)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the model is unable to infer vectors for unfamiliar words.\n",
    "This is one limitation of Word2Vec: if this limitation matters to you, check\n",
    "out the FastText model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    vec_cameroon = wv['cameroon']\n",
    "except KeyError:\n",
    "    print(\"The word 'cameroon' does not appear in this model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving on, ``Word2Vec`` supports several word similarity tasks out of the\n",
    "box.  You can see how the similarity intuitively decreases as the words get\n",
    "less and less similar.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    ('car', 'minivan'),   # a minivan is a kind of car\n",
    "    ('car', 'bicycle'),   # still a wheeled vehicle\n",
    "    ('car', 'airplane'),  # ok, no wheels, but still a vehicle\n",
    "    ('car', 'cereal'),    # ... and so on\n",
    "    ('car', 'communism'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the 5 most similar words to \"car\" or \"minivan\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wv.most_similar(positive=['car', 'minivan'], topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of the below does not belong in the sequence?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wv.doesnt_match(['fire', 'water', 'land', 'sea', 'air', 'car']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Your Own Model\n",
    "-----------------------\n",
    "\n",
    "To start, you'll need some data for training the model. For the following\n",
    "examples, we'll use the [Lee Evaluation Corpus](https://github.com/RaRe-Technologies/gensim/blob/develop/gensim/test/test_data/lee_background.cor), which you already have if you’ve installed Gensim.\n",
    "\n",
    "\n",
    "This corpus is small enough to fit entirely in memory, but we'll implement a\n",
    "memory-friendly iterator that reads it line-by-line to demonstrate how you\n",
    "would handle a larger corpus.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "\n",
    "class MyCorpus:\n",
    "    \"\"\"An iterator that yields sentences (lists of str).\"\"\"\n",
    "\n",
    "    def __iter__(self):\n",
    "        corpus_path = datapath('lee_background.cor')\n",
    "        for line in open(corpus_path):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield utils.simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to do any custom preprocessing, e.g. decode a non-standard\n",
    "encoding, lowercase, remove numbers, extract named entities... All of this can\n",
    "be done inside the ``MyCorpus`` iterator and ``word2vec`` doesn’t need to\n",
    "know. All that is required is that the input yields one sentence (list of\n",
    "utf8 words) after another.\n",
    "\n",
    "Let's go ahead and train a model on our corpus.  Don't worry about the\n",
    "training parameters much for now, we'll revisit them later.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models\n",
    "\n",
    "sentences = MyCorpus()\n",
    "model = gensim.models.Word2Vec(sentences=sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our model, we can use it in the same way as in the demo above.\n",
    "\n",
    "The main part of the model is ``model.wv`` , where \"wv\" stands for \"word vectors\".\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_king = model.wv['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vec_king)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving the vocabulary works the same way:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, word in enumerate(model.wv.index_to_key):\n",
    "    if index == 10:\n",
    "        break\n",
    "    print(f\"word #{index}/{len(model.wv.index_to_key)} is {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing and loading models\n",
    "--------------------------\n",
    "\n",
    "You'll notice that training non-trivial models can take time.  Once you've\n",
    "trained your model and it works as expected, you can save it to disk.  That\n",
    "way, you don't have to spend time training it all over again later.\n",
    "\n",
    "You can store/load models using the standard gensim methods:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'file/gensim-model-word2vec'\n",
    "\n",
    "model.save(model_path)\n",
    "\n",
    "# The model is now safely stored in the path.\n",
    "# You can copy it to other machines, share it with others, etc.\n",
    "# To load a saved model:\n",
    "\n",
    "new_model = gensim.models.Word2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In addition, you can load models created by the original C tool, both using\n",
    "its text and binary formats:\n",
    "\n",
    " <code>model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.txt', binary=False)</code>\n",
    "\n",
    "using gzipped/bz2 input works too, no need to unzip\n",
    "\n",
    " <code>model = gensim.models.KeyedVectors.load_word2vec_format('/tmp/vectors.bin.gz', binary=True)</code>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Parameters\n",
    "-------------------\n",
    "\n",
    "``Word2Vec`` accepts several parameters that affect both training speed and quality.\n",
    "\n",
    "min_count\n",
    "---------\n",
    "\n",
    "``min_count`` is for pruning the internal dictionary. Words that appear only\n",
    "once or twice in a billion-word corpus are probably uninteresting typos and\n",
    "garbage. In addition, there’s not enough data to make any meaningful training\n",
    "on those words, so it’s best to ignore them:\n",
    "\n",
    "default value of min_count=5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(sentences, min_count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vector_size\n",
    "-----------\n",
    "\n",
    "``vector_size`` is the number of dimensions (N) of the N-dimensional space that\n",
    "gensim Word2Vec maps the words onto.\n",
    "\n",
    "Bigger size values require more training data, but can lead to better (more\n",
    "accurate) models. Reasonable values are in the tens to hundreds.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The default value of vector_size is 100.\n",
    "model = gensim.models.Word2Vec(sentences, vector_size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "workers\n",
    "-------\n",
    "\n",
    "``workers`` , the last of the major parameters (full list [here](http://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vecor))\n",
    "is for training parallelization, to speed up training:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default value of workers=3 (tutorial says 1...)\n",
    "model = gensim.models.Word2Vec(sentences, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``workers`` parameter only has an effect if you have [Cython](http://cython.org/) installed. Without Cython, you’ll only be able to use\n",
    "one core because of the [GIL](https://wiki.python.org/moin/GlobalInterpreterLock) (and ``word2vec``\n",
    "training will be [miserably slow](http://rare-technologies.com/word2vec-in-python-part-two-optimizing/ ).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory\n",
    "------\n",
    "\n",
    "At its core, ``word2vec`` model parameters are stored as matrices (NumPy\n",
    "arrays). Each array is **#vocabulary** (controlled by the ``min_count`` parameter)\n",
    "times **vector size** (the ``vector_size`` parameter) of floats (single precision aka 4 bytes).\n",
    "\n",
    "Three such matrices are held in RAM (work is underway to reduce that number\n",
    "to two, or even one). So if your input contains 100,000 unique words, and you\n",
    "asked for layer ``vector_size=200``\\ , the model will require approx.\n",
    "``100,000*200*4*3 bytes = ~229MB``.\n",
    "\n",
    "There’s a little extra memory needed for storing the vocabulary tree (100,000 words would\n",
    "take a few megabytes), but unless your words are extremely loooong strings, memory\n",
    "footprint will be dominated by the three matrices above.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating\n",
    "----------\n",
    "\n",
    "``Word2Vec`` training is an unsupervised task, there’s no good way to\n",
    "objectively evaluate the result. Evaluation depends on your end application.\n",
    "\n",
    "Google has released their testing set of about 20,000 syntactic and semantic\n",
    "test examples, following the “A is to B as C is to D” task. It is provided in\n",
    "the 'datasets' folder.\n",
    "\n",
    "For example a syntactic analogy of comparative type is ``bad:worse;good:?``.\n",
    "There are total of 9 types of syntactic comparisons in the dataset like\n",
    "plural nouns and nouns of opposite meaning.\n",
    "\n",
    "The semantic questions contain five types of semantic analogies, such as\n",
    "capital cities (``Paris:France;Tokyo:?``) or family members\n",
    "(``brother:sister;dad:?``).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim supports the same evaluation set, in exactly the same format:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.wv.evaluate_word_analogies(datapath('questions-words.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ``evaluate_word_analogies`` method takes an [optional parameter](http://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.evaluate_word_analogies>)\n",
    "``restrict_vocab`` which limits which test examples are to be considered.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the December 2016 release of Gensim we added a better way to evaluate semantic similarity.\n",
    "\n",
    "By default it uses an academic dataset WS-353 but one can create a dataset\n",
    "specific to your business based on it. It contains word pairs together with\n",
    "human-assigned similarity judgments. It measures the relatedness or\n",
    "co-occurrence of two words. For example, 'coast' and 'shore' are very similar\n",
    "as they appear in the same context. At the same time 'clothes' and 'closet'\n",
    "are less similar because they are related but not interchangeable.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.wv.evaluate_word_pairs(datapath('wordsim353.tsv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "  Good performance on Google's or WS-353 test set doesn’t mean word2vec will\n",
    "  work well in your application, or vice versa. It’s always best to evaluate\n",
    "  directly on your intended task. For an example of how to use word2vec in a\n",
    "  classifier pipeline, see this [tutorial](https://github.com/RaRe-Technologies/movie-plots-by-genre)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online training / Resuming training\n",
    "-----------------------------------\n",
    "\n",
    "Advanced users can load a model and continue training it with more sentences\n",
    "and new vocabulary words.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec.load(model_path)\n",
    "more_sentences = [\n",
    "    ['Advanced', 'users', 'can', 'load', 'a', 'model',\n",
    "     'and', 'continue', 'training', 'it', 'with', 'more', 'sentences'],\n",
    "]\n",
    "model.build_vocab(more_sentences, update=True)\n",
    "model.train(more_sentences, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may need to tweak the ``total_words`` parameter to ``train()``,\n",
    "depending on what learning rate decay you want to simulate.\n",
    "\n",
    "Note that it’s not possible to resume training with models generated by the C\n",
    "tool, ``KeyedVectors.load_word2vec_format()``. You can still use them for\n",
    "querying/similarity, but information vital for training (the vocab tree) is\n",
    "missing there.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Loss Computation\n",
    "-------------------------\n",
    "\n",
    "The parameter ``compute_loss`` can be used to toggle computation of loss\n",
    "while training the Word2Vec model. The computed loss is stored in the model\n",
    "attribute ``running_training_loss`` and can be retrieved using the function\n",
    "``get_latest_training_loss`` as follows :\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating and training the Word2Vec model\n",
    "model_with_loss = gensim.models.Word2Vec(\n",
    "    sentences,\n",
    "    min_count=1,\n",
    "    compute_loss=True,\n",
    "    hs=0,\n",
    "    sg=1,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# getting the training loss value\n",
    "training_loss = model_with_loss.get_latest_training_loss()\n",
    "print(training_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualising Word Embeddings\n",
    "---------------------------\n",
    "\n",
    "The word embeddings made by the model can be visualised by reducing\n",
    "dimensionality of the words to 2 dimensions using tSNE.\n",
    "\n",
    "Visualisations can be used to notice semantic and syntactic trends in the data.\n",
    "\n",
    "Example:\n",
    "\n",
    "* Semantic: words like cat, dog, cow, etc. have a tendency to lie close by\n",
    "* Syntactic: words like run, running or cut, cutting lie close together.\n",
    "\n",
    "Vector relations like vKing - vMan = vQueen - vWoman can also be noticed.\n",
    "\n",
    "\n",
    "The model used for the visualisation is trained on a small corpus. Thus\n",
    "some of the relations might not be so clear.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA    # inital reduction\n",
    "from sklearn.manifold import TSNE                   # final reduction\n",
    "import numpy as np                                  # array handling\n",
    "\n",
    "\n",
    "def reduce_dimensions(model):\n",
    "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
    "\n",
    "    # extract the words & their vectors, as numpy arrays\n",
    "    vectors = np.asarray(model.wv.vectors)\n",
    "    labels = np.asarray(model.wv.index_to_key)  # fixed-width numpy strings\n",
    "\n",
    "    # reduce using t-SNE\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals, labels\n",
    "\n",
    "\n",
    "x_vals, y_vals, labels = reduce_dimensions(model)\n",
    "\n",
    "def plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=True):\n",
    "    from plotly.offline import init_notebook_mode, iplot, plot\n",
    "    import plotly.graph_objs as go\n",
    "\n",
    "    trace = go.Scatter(x=x_vals, y=y_vals, mode='text', text=labels)\n",
    "    data = [trace]\n",
    "\n",
    "    if plot_in_notebook:\n",
    "        init_notebook_mode(connected=True)\n",
    "        iplot(data, filename='word-embedding-plot')\n",
    "    else:\n",
    "        plot(data, filename='word-embedding-plot.html')\n",
    "\n",
    "\n",
    "def plot_with_matplotlib(x_vals, y_vals, labels):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import random\n",
    "\n",
    "    random.seed(0)\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.scatter(x_vals, y_vals)\n",
    "\n",
    "    #\n",
    "    # Label randomly subsampled 25 data points\n",
    "    #\n",
    "    indices = list(range(len(labels)))\n",
    "    selected_indices = random.sample(indices, 25)\n",
    "    for i in selected_indices:\n",
    "        plt.annotate(labels[i], (x_vals[i], y_vals[i]))\n",
    "\n",
    "try:\n",
    "    get_ipython()\n",
    "except Exception:\n",
    "    plot_function = plot_with_matplotlib\n",
    "else:\n",
    "    plot_function = plot_with_plotly\n",
    "\n",
    "plot_function(x_vals, y_vals, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "FastText Model\n",
    "==============\n",
    "\n",
    "Introduces Gensim's fastText model and demonstrates its use on the Lee Corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll learn to work with fastText library for training word-embedding\n",
    "models, saving & loading them and performing similarity operations & vector\n",
    "lookups analogous to Word2Vec.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use fastText?\n",
    "\n",
    "\n",
    "The main principle behind [fastText](<https://github.com/facebookresearch/fastText>) is that the\n",
    "morphological structure of a word carries important information about the meaning of the word.\n",
    "Such structure is not taken into account by traditional word embeddings like Word2Vec, which\n",
    "train a unique word embedding for every individual word.\n",
    "This is especially significant for morphologically rich languages (German, Turkish) in which a\n",
    "single word can have a large number of morphological forms, each of which might occur rarely,\n",
    "thus making it hard to train good word embeddings.\n",
    "\n",
    "\n",
    "fastText attempts to solve this by treating each word as the aggregation of its subwords.\n",
    "For the sake of simplicity and language-independence, subwords are taken to be the character ngrams\n",
    "of the word. The vector for a word is simply taken to be the sum of all vectors of its component char-ngrams.\n",
    "\n",
    "\n",
    "fastText can obtain vectors even for out-of-vocabulary (OOV) words, by summing up vectors for its\n",
    "component char-ngrams, provided at least one of the char-ngrams was present in the training data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training models\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following examples, we'll use the Lee Corpus (which you already have if you've installed Gensim) for training our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint as print\n",
    "from gensim.models.fasttext import FastText\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "# Set file names for train and test data\n",
    "corpus_file = datapath('lee_background.cor')\n",
    "\n",
    "model = FastText(vector_size=100)\n",
    "\n",
    "# build the vocabulary\n",
    "model.build_vocab(corpus_file=corpus_file)\n",
    "\n",
    "# train the model\n",
    "model.train(\n",
    "    corpus_file=corpus_file, epochs=model.epochs,\n",
    "    total_examples=model.corpus_count, total_words=model.corpus_total_words,\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training hyperparameters\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters for training the model follow the same pattern as Word2Vec. FastText supports the following parameters from the original word2vec:\n",
    "\n",
    "- model: Training architecture. Allowed values: `cbow`, `skipgram` (Default `cbow`)\n",
    "- vector_size: Dimensionality of vector embeddings to be learnt (Default 100)\n",
    "- alpha: Initial learning rate (Default 0.025)\n",
    "- window: Context window size (Default 5)\n",
    "- min_count: Ignore words with number of occurrences below this (Default 5)\n",
    "- loss: Training objective. Allowed values: `ns`, `hs`, `softmax` (Default `ns`)\n",
    "- sample: Threshold for downsampling higher-frequency words (Default 0.001)\n",
    "- negative: Number of negative words to sample, for `ns` (Default 5)\n",
    "- epochs: Number of epochs (Default 5)\n",
    "- sorted_vocab: Sort vocab by descending frequency (Default 1)\n",
    "- threads: Number of threads to use (Default 12)\n",
    "\n",
    "\n",
    "In addition, fastText has three additional parameters:\n",
    "\n",
    "- min_n: min length of char ngrams (Default 3)\n",
    "- max_n: max length of char ngrams (Default 6)\n",
    "- bucket: number of buckets used for hashing ngrams (Default 2000000)\n",
    "\n",
    "\n",
    "Parameters ``min_n`` and ``max_n`` control the lengths of character ngrams that each word is broken down into while training and looking up embeddings. If ``max_n`` is set to 0, or to be lesser than ``min_n`` , no character ngrams are used, and the model effectively reduces to Word2Vec.\n",
    "\n",
    "\n",
    "\n",
    "To bound the memory requirements of the model being trained, a hashing function is used that maps ngrams to integers in 1 to K. For hashing these character sequences, the [Fowler-Noll-Vo hashing function](http://www.isthe.com/chongo/tech/comp/fnv)(FNV-1a variant) is employed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving/loading models\n",
    "---------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models can be saved and loaded via the ``load`` and ``save`` methods, just like\n",
    "any other model in Gensim.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a model trained via Gensim's fastText implementation to temp.\n",
    "import tempfile\n",
    "import os\n",
    "with tempfile.NamedTemporaryFile(prefix='saved_model_gensim-', delete=False) as tmp:\n",
    "    model.save(tmp.name, separately=[])\n",
    "\n",
    "# Load back the same model.\n",
    "loaded_model = FastText.load(tmp.name)\n",
    "print(loaded_model)\n",
    "\n",
    "os.unlink(tmp.name)  # demonstration complete, don't need the temp file anymore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``save_word2vec_format`` is also available for fastText models, but will\n",
    "cause all vectors for ngrams to be lost.\n",
    "As a result, a model loaded in this way will behave as a regular word2vec model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word vector lookup\n",
    "------------------\n",
    "\n",
    "\n",
    "All information necessary for looking up fastText words (incl. OOV words) is\n",
    "contained in its ``model.wv`` attribute.\n",
    "\n",
    "If you don't need to continue training your model, you can export & save this `.wv`\n",
    "attribute and discard `model`, to save space and RAM.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = model.wv\n",
    "print(wv)\n",
    "\n",
    "# FastText models support vector lookups for out-of-vocabulary words by \n",
    "# summing up character ngrams belonging to the word.\n",
    "\n",
    "print('night' in wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('nights' in wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wv['night'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wv['nights'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity operations\n",
    "---------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity operations work the same way as word2vec. **Out-of-vocabulary words can also be used, provided they have at least one character ngram present in the training data.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"nights\" in wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"night\" in wv.key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wv.similarity(\"night\", \"nights\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntactically similar words generally have high similarity in fastText models, since a large number of the component char-ngrams will be the same. As a result, fastText generally does better at syntactic tasks than Word2Vec. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other similarity operations\n",
    "\n",
    "The example training corpus is a toy corpus, results are not expected to be good, for proof-of-concept only\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wv.most_similar(\"nights\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wv.n_similarity(['sushi', 'shop'], ['japanese', 'restaurant']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wv.doesnt_match(\"breakfast cereal dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wv.most_similar(positive=['baghdad', 'england'], negative=['london']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(wv.evaluate_word_analogies(datapath('questions-words.txt')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
